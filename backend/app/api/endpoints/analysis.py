import json
from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any
from datetime import datetime, timezone, timedelta
from sqlalchemy.orm import Session
from app.llm_agent.graphs.limit_up_stock_analysis_graph import run_ai_research_analysis
from app.core.database import get_db, SessionLocal
from app.models.stock import AnalysisReport
from app.services.ai_analysis import AIAnalysisService

router = APIRouter()

class AnalysisRequest(BaseModel):
    date: Optional[str] = None
    force_rerun: bool = False

class AnalysisResponse(BaseModel):
    success: bool
    message: str
    result: Optional[Dict[str, Any]] = None
    cached: bool = False

# ç”¨äºè·Ÿè¸ªæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ (Key: {date}_{report_type})
active_tasks = set()

def background_analysis_task(date: str, force_rerun: bool):
    """
    åå°åˆ†æä»»åŠ¡åŒ…è£…å‡½æ•°
    """
    task_key = f"{date}_limit-up"
    try:
        active_tasks.add(task_key)
        print(f"ğŸš€ å¼€å§‹åå°åˆ†æä»»åŠ¡: {task_key}")
        run_ai_research_analysis(date=date, force_rerun=force_rerun)
        print(f"âœ… åå°åˆ†æä»»åŠ¡å®Œæˆ: {task_key}")
    except Exception as e:
        print(f"âŒ åå°åˆ†æä»»åŠ¡å¤±è´¥ ({task_key}): {e}")
    finally:
        active_tasks.discard(task_key)

@router.post("/limit-up", response_model=AnalysisResponse)
async def run_limit_up_analysis(
    background_tasks: BackgroundTasks,
    request: AnalysisRequest = AnalysisRequest(),
    db: Session = Depends(get_db)
):
    """
    è¿è¡Œæ¶¨åœè‚¡åˆ†æï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰
    """
    target_date = request.date or datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d")
    task_key = f"{target_date}_limit-up"
    
    # 1. æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è¿è¡Œä¸­
    if task_key in active_tasks:
        return AnalysisResponse(
            success=True,
            message="åˆ†æä¸­ï¼Œè¯·ç­‰å¾…",
            cached=False
        )
    
    # 2. æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å®Œæˆçš„ä»»åŠ¡
    if not request.force_rerun:
        report_date = datetime.strptime(target_date, "%Y-%m-%d")
        existing = db.query(AnalysisReport).filter(
            AnalysisReport.report_date == report_date,
            AnalysisReport.report_type == "limit-up"
        ).first()
        
        if existing:
            return AnalysisResponse(
                success=True,
                message="åˆ†æå®Œæˆï¼Œè¯·æŸ¥è¯¢æŠ¥å‘Šã€‚",
                cached=True
            )
            
    # 3. å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(background_analysis_task, target_date, request.force_rerun)
    
    
    return AnalysisResponse(
        success=True,
        message="åˆ†æä¸­ï¼Œè¯·ç­‰å¾…",
        cached=False
    )

def background_opening_analysis_task(date: str, force_rerun: bool):
    """
    åå°å¼€ç›˜åˆ†æä»»åŠ¡åŒ…è£…å‡½æ•°
    """
    task_key = f"{date}_opening_analysis"
    try:
        active_tasks.add(task_key)
        print(f"ğŸš€ å¼€å§‹åå°å¼€ç›˜åˆ†æä»»åŠ¡: {task_key}")
        from app.llm_agent.graphs.opening_analysis_workflow import run_opening_analysis
        run_opening_analysis(date=date, force_rerun=force_rerun)
        print(f"âœ… åå°å¼€ç›˜åˆ†æä»»åŠ¡å®Œæˆ: {task_key}")
    except Exception as e:
        print(f"âŒ åå°å¼€ç›˜åˆ†æä»»åŠ¡å¤±è´¥ ({task_key}): {e}")
    finally:
        active_tasks.discard(task_key)

@router.post("/opening-analysis", response_model=AnalysisResponse)
async def run_opening_analysis_endpoint(
    background_tasks: BackgroundTasks,
    request: AnalysisRequest = AnalysisRequest(),
    db: Session = Depends(get_db)
):
    """
    è¿è¡Œå¼€ç›˜åˆ†æï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰
    """
    target_date = request.date or datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d")
    task_key = f"{target_date}_opening_analysis"
    
    # 1. æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è¿è¡Œä¸­
    if task_key in active_tasks:
        return AnalysisResponse(
            success=True,
            message="å¼€ç›˜åˆ†æè¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…",
            cached=False
        )
    
    # 2. æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å®Œæˆçš„ä»»åŠ¡
    if not request.force_rerun:
        report_date = datetime.strptime(target_date, "%Y-%m-%d")
        existing = db.query(AnalysisReport).filter(
            AnalysisReport.report_date == report_date,
            AnalysisReport.report_type == "opening_analysis"
        ).first()
        
        if existing:
            return AnalysisResponse(
                success=True,
                message="åˆ†æå®Œæˆï¼Œè¯·æŸ¥è¯¢æŠ¥å‘Šã€‚",
                cached=True
            )
            
    # 3. å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(background_opening_analysis_task, target_date, request.force_rerun)
    
    return AnalysisResponse(
        success=True,
        message="å¼€ç›˜åˆ†æå·²å¯åŠ¨ï¼Œè¯·ç­‰å¾…",
        cached=False
    )

@router.get("/report", response_model=AnalysisResponse)
async def get_analysis_report(
    report_type: str,
    date: str,
    symbol: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """
    æŸ¥è¯¢æŒ‡å®šç±»å‹å’Œæ—¥æœŸçš„åˆ†ææŠ¥å‘Š
    """
    try:
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼å¤„ç†
        report_date = datetime.strptime(date, "%Y-%m-%d")
        
        query = db.query(AnalysisReport).filter(
            AnalysisReport.report_date == report_date,
            AnalysisReport.report_type == report_type
        )
        if symbol:
            query = query.filter(AnalysisReport.symbol == symbol)
        
        report = query.first()
        
        if not report:
            return AnalysisResponse(
                success=False,
                message=f"æœªæ‰¾åˆ° {date} çš„ {report_type} åˆ†ææŠ¥å‘Š",
                cached=False
            )
            
        # æå–æ•°æ®
        markdown_content = report.content
        raw_data = {}
        if report.data:
            try:
                raw_data = json.loads(report.data)
            except:
                raw_data = {"message": "Could not parse JSON data"}
        
        # ç»„è£…è¿”å›ç»“æœ
        result_data = {
            "markdown": markdown_content,
            "data": raw_data,
            "date": date
        }
                
        return AnalysisResponse(
            success=True,
            message="æŸ¥è¯¢æˆåŠŸ",
            result=result_data,
            cached=True
        )
    except ValueError:
        raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class KlineAnalysisRequest(BaseModel):
    symbol: str = Field(..., description="è‚¡ç¥¨ä»£ç ")
    force_rerun: bool = False


def background_kline_analysis_task(symbol: str, days: int, force_rerun: bool):
    """
    åå°Kçº¿åˆ†æä»»åŠ¡
    """
    task_key = f"kline_{symbol}_{datetime.now().strftime('%Y%m%d')}"
    try:
        active_tasks.add(task_key)
        print(f"ğŸš€ å¼€å§‹åå°Kçº¿åˆ†æä»»åŠ¡: {task_key}")
        
        db = SessionLocal()
        try:
            service = AIAnalysisService(db)
            service.analyze_kline(symbol, days)
        finally:
            db.close()
            
        print(f"âœ… åå°Kçº¿åˆ†æä»»åŠ¡å®Œæˆ: {task_key}")
    except Exception as e:
        print(f"âŒ åå°Kçº¿åˆ†æä»»åŠ¡å¤±è´¥ ({task_key}): {e}")
    finally:
        active_tasks.discard(task_key)


@router.post("/kline", response_model=AnalysisResponse)
async def run_kline_analysis(
    background_tasks: BackgroundTasks,
    request: KlineAnalysisRequest,
    db: Session = Depends(get_db)
):
    """
    è¿è¡ŒKçº¿ AI åˆ†æï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰
    """
    task_key = f"kline_{request.symbol}_{datetime.now().strftime('%Y%m%d')}"
    
    # 1. æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è¿è¡Œä¸­
    if task_key in active_tasks:
        return AnalysisResponse(
            success=True,
            message=f"è‚¡ç¥¨ {request.symbol} åˆ†æè¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…",
            cached=False
        )
    
    # 2. æ£€æŸ¥å½“æ—¥æ˜¯å¦å·²åˆ†æ
    if not request.force_rerun:
        today = datetime.now().strftime('%Y-%m-%d')
        report_date = datetime.strptime(today, "%Y-%m-%d")
        existing = db.query(AnalysisReport).filter(
            AnalysisReport.symbol == request.symbol,
            AnalysisReport.report_date == report_date,
            AnalysisReport.report_type == 'kline_analysis'
        ).first()
        
        if existing:
            return AnalysisResponse(
                success=True,
                message="åˆ†æå·²å®Œæˆï¼Œè¯·æŸ¥è¯¢æŠ¥å‘Šã€‚",
                cached=True
            )
    
    # 3. å¯åŠ¨åå°ä»»åŠ¡ - é»˜è®¤åˆ†æ90å¤©
    background_tasks.add_task(
        background_kline_analysis_task, 
        request.symbol, 
        90, 
        request.force_rerun
    )
    
    return AnalysisResponse(
        success=True,
        message=f"å·²å¯åŠ¨ {request.symbol} Kçº¿åˆ†æ",
        cached=False
    )
