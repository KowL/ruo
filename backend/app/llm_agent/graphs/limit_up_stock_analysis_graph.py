# limit_up_stock_analysis_graph.py
"""
æ¶¨åœè‚¡åˆ†æå·¥ä½œæµå›¾

ä¸“é—¨ç”¨äºæ¶¨åœè‚¡ AI æŠ•ç ”åˆ†æçš„ LangGraph å·¥ä½œæµå®šä¹‰
é‡æ„åçš„æ¨¡å—åŒ–æ¶æ„ï¼Œä½¿ç”¨ç‹¬ç«‹çš„ state å’Œ agent æ¨¡å—
"""

from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
import json
import traceback
from pathlib import Path
import pandas as pd
from datetime import datetime

# åŠ è½½å¯†é’¥
load_dotenv()

# å¯¼å…¥æ¨¡å—åŒ–ç»„ä»¶
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from state import ResearchState
from agent import (
    node_data_officer,
    node_strategist,
    node_risk_controller,
    node_day_trading_coach,
    node_finalize_report
)
from llm_factory import get_shared_llm

# =======================
# ğŸ§­ æ¡ä»¶è·¯ç”±å‡½æ•°
# =======================
def route_next_step(state: ResearchState) -> str:
    """æ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œçš„èŠ‚ç‚¹"""
    return state["next_action"]

# =======================
# ğŸŒ æ„å»ºæ¶¨åœè‚¡åˆ†æå·¥ä½œæµå›¾
# =======================
def create_research_graph(llm=None):
    """
    åˆ›å»ºæ¶¨åœè‚¡ç ”ç©¶å·¥ä½œæµå›¾

    Args:
        llm: å¯é€‰çš„ LLM å®ä¾‹ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å…±äº«å®ä¾‹

    Returns:
        ç¼–è¯‘åçš„å·¥ä½œæµå›¾
    """
    if llm is None:
        llm = get_shared_llm()

    workflow = StateGraph[ResearchState, None, ResearchState, ResearchState](ResearchState)

    # åˆ›å»ºåŒ…è£…å‡½æ•°æ¥ä¼ é€’ LLM å®ä¾‹
    def wrapped_data_officer(state):
        return node_data_officer(state)

    def wrapped_strategist(state):
        return node_strategist(state, llm)

    def wrapped_risk_controller(state):
        return node_risk_controller(state)

    def wrapped_day_trading_coach(state):
        return node_day_trading_coach(state, llm)

    def wrapped_finalize_report(state):
        return node_finalize_report(state)

    # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    workflow.add_node("node_data_officer", wrapped_data_officer)
    workflow.add_node("node_strategist", wrapped_strategist)
    workflow.add_node("node_risk_controller", wrapped_risk_controller)
    workflow.add_node("node_day_trading_coach", wrapped_day_trading_coach)
    workflow.add_node("node_finalize_report", wrapped_finalize_report)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("node_data_officer")

    # æ·»åŠ æ¡ä»¶è¾¹ï¼ˆCondition Edgeï¼‰
    workflow.add_conditional_edges(
        "node_data_officer",
        route_next_step,
        {
            "TO_STRATEGIST": "node_strategist"
        }
    )
    workflow.add_conditional_edges(
        "node_strategist",
        route_next_step,
        {
            "TO_RISK_CONTROLLER": "node_risk_controller"
        }
    )
    workflow.add_conditional_edges(
        "node_risk_controller",
        route_next_step,
        {
            "TO_DAY_TRADING_COACH": "node_day_trading_coach"
        }
    )
    workflow.add_conditional_edges(
        "node_day_trading_coach",
        route_next_step,
        {
            "TO_FINALIZER": "node_finalize_report"
        }
    )
    workflow.add_conditional_edges(
        "node_finalize_report",
        route_next_step,
        {
            "FINISH": END
        }
    )

    # ç¼–è¯‘å›¾
    app = workflow.compile()
    return app

# === ç¼“å­˜é…ç½® ===
CACHE_DIR = Path("cache/daily_research")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# æ–°å¢å‡½æ•°ï¼šä¿å­˜æŠ¥å‘Šåˆ°æœ¬åœ°
def save_report_to_cache(state: dict, date: str):
    """å°†åˆ†æç»“æœæŒä¹…åŒ–åˆ°æœ¬åœ°ç¼“å­˜"""
    # åˆ›å»ºæ—¥æœŸå­ç›®å½•
    date_dir = CACHE_DIR / date
    date_dir.mkdir(exist_ok=True)

    # ä¿å­˜å®Œæ•´çŠ¶æ€ï¼ˆç”¨äºè°ƒè¯•å’Œåç»­å·¥ä½œæµï¼‰
    state_path = date_dir / "state.json"
    with open(state_path, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2, default=str)

    # ç”Ÿæˆå¹¶ä¿å­˜ Markdown æŠ¥å‘Šï¼ˆç»™äººçœ‹ï¼‰
    md_content = f"""
# ğŸ“Š AIæŠ•ç ”æ—¥æŠ¥ï¼š{date}

ğŸ“… åˆ†ææ—¶é—´ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
{'-'*50}

## ğŸ“ˆ æ•°æ®å®˜ç®€æŠ¥
{state.get('data_officer_report', 'æ— ')}

## ğŸ’¡ ç­–ç•¥å¸ˆè§‚ç‚¹
> {state.get('strategist_thinking', 'æ— ')}

## ğŸ›¡ï¸ é£æ§æé†’
"""
    for alert in state.get("risk_controller_alerts", []):
        md_content += f"- {alert}\n"

    md_content += "\n## ğŸ¥‹ çŸ­çº¿é¾™å¤´åŠ©æ‰‹å»ºè®®\n"
    for item in state.get("day_trading_coach_advice", []):
        if isinstance(item, dict) and "name" in item:
            md_content += f"""
### {item['name']} ({item['code']})
- **æ“ä½œå»ºè®®**ï¼š{item['action']}
- **æ¢¯é˜Ÿåœ°ä½**ï¼š{item.get('tier_rank', '?')}
- **æƒ…ç»ªå‘¨æœŸ**ï¼š{item.get('mood_cycle', '?')}
- **ç†æƒ³ä¹°ç‚¹**ï¼š{item['entry_point']}
- **æ­¢æŸä»·**ï¼š{item.get('stop_loss', '?')} å…ƒ
- **ç›®æ ‡ä»·**ï¼š{item.get('take_profit', '?')} å…ƒ
- **é£é™©æ”¶ç›Šæ¯”**ï¼š{item.get('risk_reward_ratio', '?')}
- **é£é™©ä¿¡å·**ï¼š{item.get('risk_signal', 'æ— ')}
- **é€»è¾‘**ï¼š{item['reason']}
"""

    md_content += f"\n\n---\nğŸ“Œ ç»¼åˆå»ºè®®ï¼šçŸ­çº¿é€‰æ‰‹å¯åœ¨æ§åˆ¶ä»“ä½å‰æä¸‹å‚ä¸é«˜ç¡®å®šæ€§æœºä¼š..."

    md_path = date_dir / "report.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_content.strip())

    # ï¼ˆå¯é€‰ï¼‰ä¿å­˜åŸå§‹æ•°æ®
    if "raw_limit_ups" in state:
        import pickle
        pkl_path = date_dir / "raw_data.pkl"
        with open(pkl_path, "wb") as f:
            pickle.dump(pd.DataFrame(state["raw_limit_ups"]), f)

    print(f"âœ… æŠ¥å‘Šå·²ç¼“å­˜è‡³: {date_dir}")

# æ–°å¢å‡½æ•°ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜
def is_cached(date: str) -> bool:
    """åˆ¤æ–­æŸæ—¥çš„åˆ†ææŠ¥å‘Šæ˜¯å¦å·²å­˜åœ¨"""
    date_dir = CACHE_DIR / date
    return date_dir.exists() and (date_dir / "report.md").exists()

# ä¸»å…¥å£å‡½æ•°ï¼šæ”¯æŒç¼“å­˜è¯»å–ä¸å†™å…¥
def run_ai_research_analysis(date: str, force_rerun: bool = False, llm=None) -> dict:
    """
    å¯åŠ¨å®Œæ•´çš„æ¶¨åœè‚¡ AI æŠ•ç ”åˆ†ææµç¨‹
    æ”¯æŒç¼“å­˜æœºåˆ¶ï¼šè‹¥å·²å­˜åœ¨ä¸”æœªå¼ºåˆ¶é‡è·‘ï¼Œåˆ™ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ

    Args:
        date: åˆ†ææ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DD
        force_rerun: æ˜¯å¦å¼ºåˆ¶é‡æ–°è¿è¡Œï¼Œå¿½ç•¥ç¼“å­˜
        llm: å¯é€‰çš„ LLM å®ä¾‹ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å…±äº«å®ä¾‹

    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    cache_file = CACHE_DIR / date / "state.json"

    # âœ… æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
    if not force_rerun and is_cached(date):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cached_state = json.load(f)
            return {
                "success": True,
                "result": cached_state,
                "cached": True,
                "message": f"ä½¿ç”¨ç¼“å­˜ç»“æœï¼ˆ{date}ï¼‰"
            }
        except Exception as e:
            print(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")

    # ğŸ” å¦åˆ™æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹
    try:
        graph = create_research_graph(llm)
        initial_state = {
            "date": date,
            "raw_limit_ups": [],
            "lhb_data": [],
            "f10_data": {},
            "context_notes": [],
            "next_action": "TO_DATA_OFFICER"
        }

        # æ”¶é›†å®Œæ•´çš„çŠ¶æ€ä¿¡æ¯
        accumulated_state = initial_state.copy()

        for output in graph.stream(initial_state):
            # æ›´æ–°ç´¯ç§¯çŠ¶æ€
            for node_name, node_output in output.items():
                if isinstance(node_output, dict):
                    accumulated_state.update(node_output)

            # å¦‚æœåˆ°è¾¾ç»ˆç‚¹ï¼Œä¿å­˜æœ€ç»ˆçŠ¶æ€
            if END in output:
                final_state = accumulated_state
                break
        else:
            # å¦‚æœæ²¡æœ‰åˆ°è¾¾ENDï¼Œä½¿ç”¨ç´¯ç§¯çŠ¶æ€
            final_state = accumulated_state

        if final_state is None:
            raise ValueError("å›¾æ‰§è¡Œæœªäº§ç”Ÿä»»ä½•è¾“å‡º")

        # âœ… æ‰§è¡Œå®Œæˆåç«‹å³ç¼“å­˜
        save_report_to_cache(final_state, date)

        return {
            "success": True,
            "result": final_state,
            "cached": False,
            "message": f"æ–°ç”ŸæˆæŠ¥å‘Šå¹¶å·²ç¼“å­˜"
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }

if __name__ == "__main__":
    from datetime import datetime
    today = datetime.now().strftime("%Y-%m-%d")
    run_ai_research_analysis(today, force_rerun=True)