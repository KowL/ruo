# limit_up_stock_analysis_graph.py
"""
æ¶¨åœè‚¡åˆ†æå·¥ä½œæµå›¾

ä¸“é—¨ç”¨äºæ¶¨åœè‚¡ AI æŠ•ç ”åˆ†æçš„ LangGraph å·¥ä½œæµå®šä¹‰
é‡æ„åçš„æ¨¡å—åŒ–æ¶æ„ï¼Œä½¿ç”¨ç‹¬ç«‹çš„ state å’Œ agent æ¨¡å—
"""

from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
import json
import traceback
from pathlib import Path
import pandas as pd
from datetime import datetime, timezone, timedelta

# åŠ è½½å¯†é’¥
load_dotenv()

from app.llm_agent.state import ResearchState
from app.llm_agent.agents import (
    node_data_officer,
    node_strategist,
    node_risk_controller,
    node_day_trading_coach,
    node_finalize_report
)
from app.core.llm_factory import get_shared_llm


# =======================
# ğŸ§­ æ¡ä»¶è·¯ç”±å‡½æ•°
# =======================
def route_next_step(state: ResearchState) -> str:
    """æ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œçš„èŠ‚ç‚¹"""
    return state["next_action"]

# =======================
# ğŸŒ æ„å»ºæ¶¨åœè‚¡åˆ†æå·¥ä½œæµå›¾
# =======================
def create_research_graph(llm=None):
    """
    åˆ›å»ºæ¶¨åœè‚¡ç ”ç©¶å·¥ä½œæµå›¾

    Args:
        llm: å¯é€‰çš„ LLM å®ä¾‹ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å…±äº«å®ä¾‹

    Returns:
        ç¼–è¯‘åçš„å·¥ä½œæµå›¾
    """
    if llm is None:
        llm = get_shared_llm()

    workflow = StateGraph[ResearchState, None, ResearchState, ResearchState](ResearchState)

    # åˆ›å»ºåŒ…è£…å‡½æ•°æ¥ä¼ é€’ LLM å®ä¾‹
    def wrapped_data_officer(state):
        return node_data_officer(state)

    def wrapped_strategist(state):
        return node_strategist(state, llm)

    def wrapped_risk_controller(state):
        return node_risk_controller(state)

    def wrapped_day_trading_coach(state):
        return node_day_trading_coach(state, llm)

    def wrapped_finalize_report(state):
        return node_finalize_report(state)

    # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    workflow.add_node("node_data_officer", wrapped_data_officer)
    workflow.add_node("node_strategist", wrapped_strategist)
    workflow.add_node("node_risk_controller", wrapped_risk_controller)
    workflow.add_node("node_day_trading_coach", wrapped_day_trading_coach)
    workflow.add_node("node_finalize_report", wrapped_finalize_report)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("node_data_officer")

    # æ·»åŠ æ¡ä»¶è¾¹ï¼ˆCondition Edgeï¼‰
    workflow.add_conditional_edges(
        "node_data_officer",
        route_next_step,
        {
            "TO_STRATEGIST": "node_strategist"
        }
    )
    workflow.add_conditional_edges(
        "node_strategist",
        route_next_step,
        {
            "TO_RISK_CONTROLLER": "node_risk_controller"
        }
    )
    workflow.add_conditional_edges(
        "node_risk_controller",
        route_next_step,
        {
            "TO_DAY_TRADING_COACH": "node_day_trading_coach"
        }
    )
    workflow.add_conditional_edges(
        "node_day_trading_coach",
        route_next_step,
        {
            "TO_FINALIZER": "node_finalize_report"
        }
    )
    workflow.add_conditional_edges(
        "node_finalize_report",
        route_next_step,
        {
            "FINISH": END
        }
    )

    # ç¼–è¯‘å›¾
    app = workflow.compile()
    return app

# === æ•°æ®åº“ä¸ç¼“å­˜é€»è¾‘ ===

def save_report_to_db(state: dict, date: str):
    """å°†åˆ†æç»“æœæŒä¹…åŒ–åˆ°æ•°æ®åº“"""
    # ç”Ÿæˆ Markdown æŠ¥å‘Šå†…å®¹ï¼ˆç”¨äºå…¼å®¹è€ç‰ˆæœ¬æˆ–ç›´æ¥å±•ç¤ºï¼‰
    md_content = f"""
# ğŸ“Š AIæŠ•ç ”æ—¥æŠ¥ï¼š{date}

ğŸ“… åˆ†ææ—¶é—´ï¼š{datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S")}
{'-'*50}

## ğŸ“ˆ æ•°æ®å®˜ç®€æŠ¥
{state.get('data_officer_report', 'æ— ')}

## ğŸ’¡ ç­–ç•¥å¸ˆè§‚ç‚¹
> {state.get('strategist_thinking', 'æ— ')}

## ğŸ›¡ï¸ é£æ§æé†’
"""
    for alert in state.get("risk_controller_alerts", []):
        md_content += f"- {alert}\n"

    md_content += "\n## ğŸ¥‹ çŸ­çº¿é¾™å¤´åŠ©æ‰‹å»ºè®®\n"
    for item in state.get("day_trading_coach_advice", []):
        if isinstance(item, dict) and "name" in item:
            md_content += f"""
### {item['name']} ({item['code']})
- **æ“ä½œå»ºè®®**ï¼š{item['action']}
- **é€»è¾‘**ï¼š{item['reason']}
"""

    md_content += f"\n\n---\nğŸ“Œ ç»¼åˆå»ºè®®ï¼šçŸ­çº¿é€‰æ‰‹å¯åœ¨æ§åˆ¶ä»“ä½å‰æä¸‹å‚ä¸é«˜ç¡®å®šæ€§æœºä¼š..."

    # âœ… æ•°æ®åº“æŒä¹…åŒ–
    try:
        from app.core.database import SessionLocal
        from app.models.stock import AnalysisReport
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼å¤„ç†
        report_date = datetime.strptime(date, "%Y-%m-%d")
        
        db = SessionLocal()
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = db.query(AnalysisReport).filter(
                AnalysisReport.symbol == "GLOBAL",
                AnalysisReport.report_date == report_date,
                AnalysisReport.report_type == "limit-up"
            ).first()
            
            # å°†å®Œæ•´çš„ state åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²å­˜å…¥ content
            # è¿™æ ·å‰ç«¯æ”¶åˆ°çš„ content å°±æ˜¯å®Œæ•´çš„åˆ†æç»“æœï¼Œè€Œä¸ä»…ä»…æ˜¯ md
            state_json = json.dumps(state, ensure_ascii=False, indent=2, default=str)
            
            if existing:
                existing.content = md_content.strip()
                existing.data = state_json
                existing.summary = state.get('data_officer_report', '')
            else:
                new_report = AnalysisReport(
                    symbol="GLOBAL",
                    report_date=report_date,
                    report_type="limit-up",
                    content=md_content.strip(),
                    data=state_json,
                    summary=state.get('data_officer_report', ''),
                    confidence=1.0
                )
                db.add(new_report)
            
            db.commit()
            print(f"âœ… æŠ¥å‘Šå·²åŒæ­¥åˆ°æ•°æ®åº“: {date} (GLOBAL/limit-up)")
        except Exception as db_err:
            db.rollback()
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {db_err}")
        finally:
            db.close()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“ä¿å­˜å¼‚å¸¸: {e}")

def get_cached_report(date: str) -> dict:
    """ä»æ•°æ®åº“è·å–å·²å­˜åœ¨çš„åˆ†ææŠ¥å‘Š"""
    try:
        from app.core.database import SessionLocal
        from app.models.stock import AnalysisReport
        
        report_date = datetime.strptime(date, "%Y-%m-%d")
        db = SessionLocal()
        try:
            report = db.query(AnalysisReport).filter(
                AnalysisReport.symbol == "GLOBAL",
                AnalysisReport.report_date == report_date,
                AnalysisReport.report_type == "limit-up"
            ).first()
            
            if report and report.content:
                try:
                    return json.loads(report.content)
                except:
                    return None
            return None
        finally:
            db.close()
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®åº“ç¼“å­˜å¼‚å¸¸: {e}")
        return None

# ä¸»å…¥å£å‡½æ•°ï¼šå®Œå…¨ä¾èµ–æ•°æ®åº“
def run_ai_research_analysis(date: str, force_rerun: bool = False, llm=None) -> dict:
    """
    å¯åŠ¨å®Œæ•´çš„æ¶¨åœè‚¡ AI æŠ•ç ”åˆ†ææµç¨‹
    """
    # âœ… æ£€æŸ¥æ•°æ®åº“ç¼“å­˜æ˜¯å¦å­˜åœ¨
    if not force_rerun:
        cached_state = get_cached_report(date)
        if cached_state:
            return {
                "success": True,
                "result": cached_state,
                "cached": True,
                "message": f"ä½¿ç”¨æ•°æ®åº“ç¼“å­˜ç»“æœï¼ˆ{date}ï¼‰"
            }

    # ğŸ” å¦åˆ™æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹
    try:
        graph = create_research_graph(llm)
        initial_state = {
            "date": date,
            "raw_limit_ups": [],
            "lhb_data": [],
            "f10_data": {},
            "context_notes": [],
            "next_action": "TO_DATA_OFFICER"
        }

        # æ”¶é›†å®Œæ•´çš„çŠ¶æ€ä¿¡æ¯
        accumulated_state = initial_state.copy()

        for output in graph.stream(initial_state):
            # æ›´æ–°ç´¯ç§¯çŠ¶æ€
            for node_name, node_output in output.items():
                if isinstance(node_output, dict):
                    accumulated_state.update(node_output)

            # å¦‚æœåˆ°è¾¾ç»ˆç‚¹ï¼Œä¿å­˜æœ€ç»ˆçŠ¶æ€
            if END in output:
                final_state = accumulated_state
                break
        else:
            # å¦‚æœæ²¡æœ‰åˆ°è¾¾ENDï¼Œä½¿ç”¨ç´¯ç§¯çŠ¶æ€
            final_state = accumulated_state

        if final_state is None:
            raise ValueError("å›¾æ‰§è¡Œæœªäº§ç”Ÿä»»ä½•è¾“å‡º")

        # âœ… æ‰§è¡Œå®Œæˆåç«‹å³å­˜å…¥æ•°æ®åº“
        save_report_to_db(final_state, date)

        return {
            "success": True,
            "result": final_state,
            "cached": False,
            "message": f"æ–°ç”ŸæˆæŠ¥å‘Šå¹¶å·²å­˜å…¥æ•°æ®åº“"
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }

if __name__ == "__main__":
    from datetime import datetime
    today = datetime.now().strftime("%Y-%m-%d")
    run_ai_research_analysis(today, force_rerun=True)